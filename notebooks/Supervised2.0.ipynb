{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f11cfcd-7fcf-4042-935f-578e9855dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Machine Learning and NLP Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Sentiment Analysis (optional)\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "031c06fd-4f16-403c-94b1-ffcd6a8873d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "\n",
    "# Query to identify all unique publiation values in the database\n",
    "import duckdb\n",
    "csv_path    = \"../data/raw/newspapers/all-the-news-2-1.csv\"\n",
    "# Query to extract all unique publication values\n",
    "publishers = duckdb.query(f\"\"\"\n",
    "    SELECT\n",
    "      publication,\n",
    "      COUNT(*) AS n_articles\n",
    "    FROM read_csv_auto('{csv_path}')\n",
    "    GROUP BY publication\n",
    "    ORDER BY n_articles DESC\n",
    "\"\"\").to_df()\n",
    "publishers = publishers.dropna(subset=['publication'])\n",
    "publishers = publishers[publishers['publication'] != 'None']\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load News Data\n",
    "    df_news = pd.concat([pd.read_csv(\n",
    "        f\"../data/processed/newspapers/sample_{re.sub(r'[^\\w]+','_', pub.lower()).strip('_')}.csv\"\n",
    "    ) for pub in publishers['publication']], ignore_index=True)\n",
    "    \n",
    "    # Load Industrial Production Index\n",
    "    indpro_path = 'https://fred.stlouisfed.org/graph/fredgraph.csv?id=INDPRO'\n",
    "    df_indpro = pd.read_csv(indpro_path, parse_dates=['observation_date'])\n",
    "    df_indpro.rename(columns={'observation_date': 'date', 'INDPRO': 'ipi'}, inplace=True)\n",
    "    \n",
    "    return df_news, df_indpro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2206a85-81d6-4f23-bf32-db402a0cbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Engineering\n",
    "def advanced_feature_extraction(df_news, df_indpro):\n",
    "    # Sentiment Analysis Function\n",
    "    def get_sentiment(text):\n",
    "        return TextBlob(str(text)).sentiment.polarity\n",
    "    \n",
    "    # Preprocess and merge data\n",
    "    df_news['date'] = pd.to_datetime(df_news['date'], errors='coerce')\n",
    "    df_indpro['date'] = pd.to_datetime(df_indpro['date'], errors='coerce')\n",
    "    df_news['month'] = df_news['date'].dt.to_period('M').dt.to_timestamp()\n",
    "    df_indpro['month'] = df_indpro['date'].dt.to_period('M').dt.to_timestamp()\n",
    "    \n",
    "    # Aggregate articles by month\n",
    "    df_monthly_news = df_news.groupby('month')['article'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "    \n",
    "    # Merge with IPI\n",
    "    df_merged = pd.merge(df_monthly_news, df_indpro[['month', 'ipi']], on='month')\n",
    "    \n",
    "    # Additional Features\n",
    "    df_merged['word_count'] = df_merged['article'].apply(lambda x: len(str(x).split()))\n",
    "    df_merged['sentiment_score'] = df_merged['article'].apply(get_sentiment)\n",
    "    df_merged['month_num'] = df_merged['month'].dt.month\n",
    "    df_merged['year'] = df_merged['month'].dt.year\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3f0cfde-21aa-4961-935c-1e2dfc9bd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Text Preprocessing\n",
    "def clean_text(text_input):\n",
    "    text_input = str(text_input).lower()\n",
    "    text_input = re.sub(r'\\d+', '', text_input)\n",
    "    text_input = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text_input)\n",
    "    text_input = re.sub(r'\\s+', ' ', text_input).strip()\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e229abed-4075-4d37-800d-d593c7181740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Extraction\n",
    "def extract_features(df_merged):\n",
    "    # Topic keywords for filtering\n",
    "    topic_keywords = [\n",
    "        \"manufacturing\", \"factory\", \"production\", \"industry\", \"output\",\n",
    "        \"supply chain\", \"logistics\", \"transport\", \"shortage\"\n",
    "    ]\n",
    "    \n",
    "# Filter articles by keywords\n",
    "    def filter_keywords(text, keywords):\n",
    "        text = text.lower()\n",
    "        return ' '.join([word for word in text.split() if any(k in word for k in keywords)])\n",
    "    \n",
    "    df_merged['filtered_content'] = df_merged['article'].apply(lambda x: filter_keywords(x, topic_keywords))\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=10,\n",
    "        stop_words='english',\n",
    "        token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "        preprocessor=clean_text\n",
    "    )\n",
    "    \n",
    "    # Generate feature matrix\n",
    "    X = vectorizer.fit_transform(df_merged['filtered_content'].fillna(''))\n",
    "    y = df_merged['ipi'].values\n",
    "    \n",
    "    return X, y, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a73baada-b2a0-43ef-b850-1a8b4b5ef220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Evaluation\n",
    "def comprehensive_evaluation(y_true, y_pred):\n",
    "    return {\n",
    "        'RMSE': mean_squared_error(y_true, y_pred, squared=False),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba0db987-3597-46e6-8fe7-43c47044de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualization\n",
    "def advanced_result_visualization(y_true, y_pred):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_true, y_pred)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.title('Predicted vs Actual')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = y_true - y_pred\n",
    "    plt.scatter(y_pred, residuals)\n",
    "    plt.title('Residual Plot')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5201809-1ca0-4c7a-a6e9-55f6cbe4d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Main Execution\n",
    "def main():\n",
    "    # Load Data\n",
    "    df_news, df_indpro = load_and_preprocess_data()\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df_merged = advanced_feature_extraction(df_news, df_indpro)\n",
    "    \n",
    "    # Extract Features\n",
    "    X, y, vectorizer = extract_features(df_merged)\n",
    "    \n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Define Models with Pipelines\n",
    "    models = {\n",
    "        \"Ridge\": Pipeline([\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "            ('regressor', Ridge(alpha=1.0))\n",
    "        ]),\n",
    "        \"Lasso\": Pipeline([\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "            ('regressor', Lasso(alpha=0.1))\n",
    "        ]),\n",
    "        \"Random Forest\": Pipeline([\n",
    "            ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "        ]),\n",
    "        \"Gradient Boosting\": Pipeline([\n",
    "            ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "        ]),\n",
    "        \"SVR\": Pipeline([\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "            ('regressor', SVR(kernel='rbf', C=1.0, epsilon=0.2))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Perform Grid Search for Random Forest\n",
    "    rf_param_grid = {\n",
    "        'regressor__n_estimators': [50, 100, 200],\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # Create a separate Random Forest pipeline for grid search\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_pipeline, \n",
    "        param_grid=rf_param_grid, \n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print Grid Search Results\n",
    "    print(\"\\nBest Random Forest Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"Best Score:\", -grid_search.best_score_)\n",
    "    \n",
    "    # Update Random Forest model with best parameters\n",
    "    best_rf_params = grid_search.best_params_\n",
    "    models[\"Random Forest\"] = Pipeline([\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators=best_rf_params.get('regressor__n_estimators', 100),\n",
    "            max_depth=best_rf_params.get('regressor__max_depth', None),\n",
    "            min_samples_split=best_rf_params.get('regressor__min_samples_split', 2),\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train and Evaluate Models\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\n{name} Model Results:\")\n",
    "        eval_metrics = comprehensive_evaluation(y_test, y_pred)\n",
    "        for metric, value in eval_metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        \n",
    "        results[name] = eval_metrics\n",
    "    \n",
    "    # Visualization\n",
    "    best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
    "    best_model = models[best_model_name]\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    advanced_result_visualization(y_test, y_pred_best)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0dc427-58c6-4dcb-a8d6-5dcfa7b021e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
