{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04b394e",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Outline:\n",
    "- 1) LDA (full sample)\n",
    "- 2) BERTopic \n",
    "\n",
    "To be further developed... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6eb94",
   "metadata": {},
   "source": [
    "## 1) LDA (full sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load publishers data from the corresponding file\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb24ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# compute topic modeling for all publishers (append all the samples)\n",
    "df_all = pd.concat([pd.read_csv(f\"../data/processed/newspapers/sample_{re.sub(r'\\\\W+','_ ', pub.lower()).strip('_')}.csv\") for pub in publishers['publication']], ignore_index=True)\n",
    "\n",
    "# Tokenization and stopword removal using regex and sklearn stopwords\n",
    "custom_stopwords = ENGLISH_STOP_WORDS.union({'said', 'mr', 'also'})\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    # Keep words with 3 or more alphabetic characters\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return [t for t in tokens if t not in custom_stopwords]\n",
    "\n",
    "df_all['tokens'] = df_all['article'].apply(tokenize_and_clean)\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df_all['tokens'])\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in df_all['tokens']]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 10\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Plot wordclouds\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    topic_words = dict(lda_model.show_topic(idx, 50))\n",
    "    wc = WordCloud(width=500, height=300, background_color='white', max_words=50)\n",
    "    wc.generate_from_frequencies(topic_words)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f'Topic {idx + 1}', fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('LDA Topics – Word Clouds', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b009055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDA model and dictionary\n",
    "lda_model.save(\"../models/topic_model/lda_model.gensim\")\n",
    "dictionary.save(\"../models/topic_model/lda_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Load model and dictionary\n",
    "lda_model = models.LdaModel.load(\"../models/topic_model/lda_model.gensim\")\n",
    "dictionary = corpora.Dictionary.load(\"../models/topic_model/lda_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb79eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df_all['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f'Coherence Score (c_v): {coherence_score:.4f}')\n",
    "# A score above 0.4 is generally considered good for topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# --- Assumes these exist ---\n",
    "# - df['tokens'] = list of preprocessed word tokens\n",
    "# - df['date'] = parsed datetime\n",
    "# - df['publication'] = publisher name\n",
    "# - lda_model = trained gensim LdaModel\n",
    "# - dictionary = gensim Dictionary used to train the model\n",
    "\n",
    "# Load all samples from new processed newspapers folder\n",
    "df_all = pd.concat([pd.read_csv(f\"../data/processed/newspapers/sample_{re.sub(r'\\\\W+','_ ', pub.lower()).strip('_')}.csv\") for pub in publishers['publication']], ignore_index=True)\n",
    "\n",
    "# Step 1: Convert tokens to bag-of-words\n",
    "corpus = [dictionary.doc2bow(text) for text in df_all['tokens']]\n",
    "\n",
    "# Step 2: Get topic distribution for each article\n",
    "def get_topic_dist(bow):\n",
    "    # Return full-length vector with zero entries where necessary\n",
    "    dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    return [prob for _, prob in dist]\n",
    "\n",
    "df_all['topic_distribution'] = [get_topic_dist(doc) for doc in corpus]\n",
    "\n",
    "# Step 3: Unpack topic distributions into separate columns\n",
    "num_topics = lda_model.num_topics\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topics = pd.DataFrame(df_all['topic_distribution'].tolist(), columns=topic_cols)\n",
    "\n",
    "# Step 4: Combine with metadata\n",
    "df_meta = df_all[['date', 'publication']].copy()\n",
    "df_combined = pd.concat([df_meta, df_topics], axis=1)\n",
    "df_combined['month'] = pd.to_datetime(df_combined['date'], format='mixed', errors='coerce').dt.to_period('M')\n",
    "\n",
    "# Step 5: Aggregate topic shares by month and publisher\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# Step 6: Save to CSV\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher.csv', index=False)\n",
    "\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6286b3f-4138-48ec-8dfa-94a8930a2318",
   "metadata": {},
   "source": [
    "## 2) Supervised feature engineering\n",
    "\n",
    "Goal: Pre-define topics relevant to our context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9414a1-9850-4622-a6f1-e3b703219783",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/newspapers/sample_reuters.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, pub\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate all article samples\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43m[\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/newspapers/sample_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msafe_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpub\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpublishers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m]\u001b[49m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 2. Define keyword filter ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m topic_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupply chain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, pub\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate all article samples\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/newspapers/sample_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msafe_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpub\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m publishers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m ], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 2. Define keyword filter ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m topic_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupply chain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/newspapers/sample_reuters.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# --- 1. Load publisher article data ---\n",
    "# Load publishers\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "\n",
    "# Define safe filename generator\n",
    "def safe_filename(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "# Concatenate all article samples\n",
    "df_all = pd.concat([\n",
    "    pd.read_csv(f\"../data/processed/newspapers/sample_{safe_filename(pub)}.csv\")\n",
    "    for pub in publishers['publication']\n",
    "], ignore_index=True)\n",
    "\n",
    "# --- 2. Define keyword filter ---\n",
    "topic_keywords = [\n",
    "    \"manufacturing\", \"factory\", \"production\", \"industry\", \"output\",\n",
    "    \"supply chain\", \"logistics\", \"transport\", \"shortage\"\n",
    "]\n",
    "\n",
    "# --- 3. Clean and filter text ---\n",
    "def clean_text(text_input):\n",
    "    text_input = str(text_input).lower()\n",
    "    text_input = re.sub(r'\\d+', '', text_input)\n",
    "    text_input = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text_input)\n",
    "    text_input = re.sub(r'\\s+', ' ', text_input).strip()\n",
    "    return text_input\n",
    "\n",
    "def filter_keywords(text, keywords):\n",
    "    text = text.lower()\n",
    "    return ' '.join([\n",
    "        word for word in text.split()\n",
    "        if any(k in word for k in keywords)\n",
    "    ])\n",
    "\n",
    "# --- 4. Aggregate articles per month ---\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "df_monthly = df_all.groupby('month')['article'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# --- 5. Apply keyword filtering ---\n",
    "df_monthly['filtered_content'] = df_monthly['article'].apply(lambda x: filter_keywords(x, topic_keywords))\n",
    "\n",
    "# --- 6. Load IPI data and merge ---\n",
    "df_indpro = pd.read_csv('https://fred.stlouisfed.org/graph/fredgraph.csv?id=INDPRO', parse_dates=['observation_date'])\n",
    "df_indpro.rename(columns={'observation_date': 'date', 'INDPRO': 'ipi'}, inplace=True)\n",
    "df_indpro['month'] = df_indpro['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "df_keywords_monthly = pd.merge(df_monthly, df_indpro[['month', 'ipi']], on='month', how='inner')\n",
    "\n",
    "# --- 7. TF-IDF Vectorization ---\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=10,\n",
    "    stop_words='english',\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df_keywords_monthly['filtered_content'].fillna(''))\n",
    "\n",
    "df_features = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=[f\"kw_{word}\" for word in vectorizer.get_feature_names_out()]\n",
    ")\n",
    "df_features['month'] = df_keywords_monthly['month'].values\n",
    "df_features['ipi'] = df_keywords_monthly['ipi'].values\n",
    "\n",
    "# --- 8. Save final DataFrame ---\n",
    "df_features.to_csv(\"../data/processed/df_industry_keywords_monthly.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52ae05",
   "metadata": {},
   "source": [
    "## 3) BERTopic\n",
    "\n",
    "Goal: Topic shares by month-publisher using BERTopic for the topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73ad9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 26/26 [00:08<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from difflib import get_close_matches\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load publishers\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "\n",
    "# Get all sample files\n",
    "data_dir = \"../data/processed/newspapers/\"\n",
    "available_files = os.listdir(data_dir)\n",
    "\n",
    "# Extract clean basenames\n",
    "available_basenames = {\n",
    "    re.sub(r'^sample_|\\.csv$', '', fname): fname\n",
    "    for fname in available_files\n",
    "    if fname.startswith(\"sample_\") and fname.endswith(\".csv\")\n",
    "}\n",
    "\n",
    "# Helper to sanitize\n",
    "def sanitize(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "# Load and concat all files\n",
    "dfs = []\n",
    "for pub in tqdm(publishers['publication'], desc=\"Loading data\"):\n",
    "    pub_clean = sanitize(pub)\n",
    "    match = get_close_matches(pub_clean, available_basenames.keys(), n=1, cutoff=0.7)\n",
    "    if match:\n",
    "        matched_filename = os.path.join(data_dir, available_basenames[match[0]])\n",
    "        dfs.append(pd.read_csv(matched_filename))\n",
    "    else:\n",
    "        print(f\"No file found for: {pub}\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Clean articles\n",
    "def clean_text(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_all['clean_article'] = df_all['article'].astype(str).apply(clean_text)\n",
    "df_all = df_all[df_all['clean_article'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94297f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17d9d9a33574e738473f985d162b56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 21:38:29,745 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Fitting BERTopic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2025-06-09 21:42:14,514 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-09 21:42:14,519 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-09 21:42:14,720 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-09 21:42:14,751 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-09 21:43:00,574 - BERTopic - Representation - Completed ✓\n",
      "2025-06-09 21:43:01,608 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 🧹 Text data\n",
    "texts = df_all['clean_article'].tolist()\n",
    "\n",
    "# ⚡ Embedding model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# UMAP and KMeans\n",
    "umap_model = UMAP(n_components=5, random_state=42)\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=10, random_state=42) # choose number of clusters based for easier comparison with LDA\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=kmeans_model,\n",
    "    embedding_model=None,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print(\"Fitting BERTopic...\")\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# Save results\n",
    "topic_model.save(\"../models/topic_model/bertopic_model\")\n",
    "np.save(\"../models/topic_model/bertopic_embeddings.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 'monthly_topic_shares_by_publisher_bertopic.csv'\n"
     ]
    }
   ],
   "source": [
    "# Get number of real topics\n",
    "topic_ids = sorted([t for t in topic_model.get_topics().keys() if t != -1])\n",
    "topic_cols = [f\"topic_{i}\" for i in topic_ids]\n",
    "\n",
    "df_meta = df_all[['date', 'publication']].reset_index(drop=True)\n",
    "df_combined = df_meta.copy()\n",
    "df_combined['topic'] = topics\n",
    "\n",
    "# Extract month\n",
    "df_combined['month'] = pd.to_datetime(df_combined['date'], format='mixed', errors='coerce').dt.to_period('M')\n",
    "\n",
    "# One-hot encode topics\n",
    "df_onehot = pd.get_dummies(df_combined['topic'], prefix='topic')\n",
    "\n",
    "# Combine with meta\n",
    "df_combined = pd.concat([df_combined[['month', 'publication']], df_onehot], axis=1)\n",
    "\n",
    "# Group by month + publisher and average (which is topic share)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication']).mean().reset_index()\n",
    "\n",
    "# Save result\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_bertopic.csv', index=False)\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_bertopic.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaaf743",
   "metadata": {},
   "source": [
    "## 4) Mini-LLM Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5eee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b75bbd15aa14e89942ab3809f72250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 'monthly_topic_shares_by_publisher_minillm.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Clean text ---\n",
    "def tokenize_and_clean(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    custom_stopwords = {'said', 'mr', 'also'}  # add more if needed\n",
    "    return ' '.join([t for t in tokens if t not in custom_stopwords])\n",
    "\n",
    "df_all['clean_text'] = df_all['article'].apply(tokenize_and_clean)\n",
    "\n",
    "# --- Step 2: Generate embeddings ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df_all['clean_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# --- Step 3: Cluster into topics ---\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "df_all['topic_cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# --- Step 4: Topic distribution per document ---\n",
    "similarities = cosine_similarity(embeddings, kmeans.cluster_centers_)\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topic_dist = pd.DataFrame(similarities, columns=topic_cols)\n",
    "df_all = pd.concat([df_all, df_topic_dist], axis=1)\n",
    "\n",
    "# --- Step 5: Aggregate over time and publisher ---\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M')\n",
    "\n",
    "df_combined = pd.concat([df_all[['month', 'publication']], df_all[topic_cols]], axis=1)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# --- Step 6: Save ---\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_minillm.csv', index=False)\n",
    "\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_minillm.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6bebf",
   "metadata": {},
   "source": [
    "## 5) LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a460a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 26/26 [00:08<00:00,  3.11it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed4adbd12574ea49460a17ae0c6b124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0064bb3046451baedfee4f39d92399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3c86dae1d14afabc3506fb2780d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501228438bc24ba588772e79325b667c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4114a5d69444fe38789caab4a110f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a46626c028647469f2b87a27c79ed7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9539ccc4bf434b7ca65e0a2f2ff7cee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "  0%|          | 2/254633 [00:15<531:27:19,  7.51s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 66/254633 [02:05<134:59:45,  1.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m     72\u001b[39m tqdm.pandas()\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m df_all[\u001b[33m'\u001b[39m\u001b[33mtopics\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_all\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclean_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# --- Step 4: Embed topics and cluster ---\u001b[39;00m\n\u001b[32m     77\u001b[39m embed_model = SentenceTransformer(\u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/tqdm/std.py:917\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[39m\u001b[34m(df, func, *args, **kwargs)\u001b[39m\n\u001b[32m    914\u001b[39m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    919\u001b[39m     t.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/tqdm/std.py:912\u001b[39m, in \u001b[36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    907\u001b[39m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[32m    908\u001b[39m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[32m    911\u001b[39m     t.update(n=\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.total \u001b[38;5;129;01mor\u001b[39;00m t.n < t.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mgenerate_topics\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_topics\u001b[39m(text):\n\u001b[32m     67\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtract 10 relevant topics or keywords from the following text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTopics:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     output = \u001b[43mtopic_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     response = output[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:186\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    158\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    160\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    188\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    189\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    190\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    191\u001b[39m     ):\n\u001b[32m    192\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1431\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1424\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1425\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1428\u001b[39m         )\n\u001b[32m   1429\u001b[39m     )\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1438\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1436\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1437\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1439\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:215\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    213\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2616\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2615\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2616\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2626\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2627\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2628\u001b[39m         batch_size=batch_size,\n\u001b[32m   2629\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2635\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2636\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:4136\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4131\u001b[39m \u001b[38;5;66;03m# g. Prepare remaining data for the next iteration, including computing the stopping condition for\u001b[39;00m\n\u001b[32m   4132\u001b[39m \u001b[38;5;66;03m# beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\u001b[39;00m\n\u001b[32m   4133\u001b[39m \n\u001b[32m   4134\u001b[39m \u001b[38;5;66;03m# pluck the cache from the beam indices that will be used in the next iteration\u001b[39;00m\n\u001b[32m   4135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4136\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flatten_beam_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunning_beam_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4141\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   4142\u001b[39m this_peer_finished = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._beam_search_has_unfinished_sequences(\n\u001b[32m   4143\u001b[39m     running_beam_scores,\n\u001b[32m   4144\u001b[39m     beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m     length_penalty,\n\u001b[32m   4152\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3675\u001b[39m, in \u001b[36mGenerationMixin._temporary_reorder_cache\u001b[39m\u001b[34m(self, past_key_values, beam_idx)\u001b[39m\n\u001b[32m   3672\u001b[39m     past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\u001b[32m   3673\u001b[39m \u001b[38;5;66;03m# Standard code path: use the `Cache.reorder_cache`\u001b[39;00m\n\u001b[32m   3674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3675\u001b[39m     \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3676\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m past_key_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:1624\u001b[39m, in \u001b[36mEncoderDecoderCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, beam_idx: torch.LongTensor):\n\u001b[32m   1623\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     \u001b[38;5;28mself\u001b[39m.cross_attention_cache.reorder_cache(beam_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/project-ds4pp-eth2025/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:183\u001b[39m, in \u001b[36mCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx].numel():\n\u001b[32m    182\u001b[39m     device = \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx].device\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx].numel():\n\u001b[32m    185\u001b[39m     device = \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx].device\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Step 1: Load and clean data ---\n",
    "\n",
    "# Load publishers\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "\n",
    "# Get all sample files\n",
    "data_dir = \"../data/processed/newspapers/\"\n",
    "available_files = os.listdir(data_dir)\n",
    "\n",
    "# Extract clean basenames\n",
    "available_basenames = {\n",
    "    re.sub(r'^sample_|\\.csv$', '', fname): fname\n",
    "    for fname in available_files\n",
    "    if fname.startswith(\"sample_\") and fname.endswith(\".csv\")\n",
    "}\n",
    "\n",
    "# Helper to sanitize publication names\n",
    "def sanitize(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "# Load and concat all files\n",
    "dfs = []\n",
    "for pub in tqdm(publishers['publication'], desc=\"Loading data\"):\n",
    "    pub_clean = sanitize(pub)\n",
    "    match = get_close_matches(pub_clean, available_basenames.keys(), n=1, cutoff=0.7)\n",
    "    if match:\n",
    "        matched_filename = os.path.join(data_dir, available_basenames[match[0]])\n",
    "        dfs.append(pd.read_csv(matched_filename))\n",
    "    else:\n",
    "        print(f\"No file found for: {pub}\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Clean articles: keep only words >=3 letters\n",
    "def clean_text(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_all['clean_article'] = df_all['article'].astype(str).apply(clean_text)\n",
    "df_all = df_all[df_all['clean_article'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# --- Step 2: Further clean for topic generation ---\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    custom_stopwords = {'said', 'mr', 'also'}\n",
    "    return ' '.join([t for t in tokens if t not in custom_stopwords])\n",
    "\n",
    "df_all['clean_text'] = df_all['article'].astype(str).apply(tokenize_and_clean)\n",
    "\n",
    "# --- Step 3: Setup tokenizer and model with truncation ---\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(device)\n",
    "\n",
    "topic_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device != 'cpu' else -1,\n",
    "    max_new_tokens=30\n",
    ")\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "\n",
    "def truncate_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > MAX_INPUT_TOKENS:\n",
    "        tokens = tokens[:MAX_INPUT_TOKENS]\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "def generate_topics(text):\n",
    "    truncated = truncate_text(text)\n",
    "    prompt = f\"Extract 10 relevant topics or keywords from the following text:\\n\\n{truncated}\\n\\nTopics:\"\n",
    "    output = topic_pipe(prompt, do_sample=False)\n",
    "    response = output[0]['generated_text']\n",
    "    topics = response.split(\"Topics:\")[-1].strip()\n",
    "    return topics\n",
    "\n",
    "tqdm.pandas()\n",
    "df_all['topics'] = df_all['clean_text'].progress_apply(generate_topics)\n",
    "\n",
    "# --- Step 4: Embed topics and cluster ---\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode(df_all['topics'].tolist(), show_progress_bar=True)\n",
    "\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "df_all['topic_cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# --- Step 5: Topic distribution per document ---\n",
    "\n",
    "similarities = cosine_similarity(embeddings, kmeans.cluster_centers_)\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topic_dist = pd.DataFrame(similarities, columns=topic_cols)\n",
    "df_all = pd.concat([df_all, df_topic_dist], axis=1)\n",
    "\n",
    "# --- Step 6: Aggregate over time and publisher ---\n",
    "\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M')\n",
    "\n",
    "df_combined = pd.concat([df_all[['month', 'publication']], df_all[topic_cols]], axis=1)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# --- Step 7: Save ---\n",
    "\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_flan_t5.csv', index=False)\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_flan_t5.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a2d6e",
   "metadata": {},
   "source": [
    "## 6) Supervised LDA (sLDA) Implementation with Tomotopy\n",
    "\n",
    "Unlike the previous approach that used a two-stage method (sklearn LDA + Linear Regression), we'll implement true supervised LDA using tomotopy, which integrates the supervision signal (IPI data) directly into the topic modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tomotopy as tp  # We're using tomotopy for supervised LDA\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3068f",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's load the pre-processed newspaper data and the IPI (supervision) data for sLDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ca27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load publishers information\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "print(f\"Loaded information for {len(publishers)} publishers\")\n",
    "\n",
    "# Load all samples from processed newspapers folder\n",
    "# Create a function to convert publisher names to filenames\n",
    "def publisher_to_filename(publisher_name):\n",
    "    # Replace special characters with underscores\n",
    "    return f\"sample_{re.sub(r'\\W+', '_', publisher_name.lower()).strip('_')}.csv\"\n",
    "\n",
    "# Load all sample files\n",
    "df_all = pd.concat([\n",
    "    pd.read_csv(f\"../data/processed/newspapers/{publisher_to_filename(pub)}\")\n",
    "    for pub in publishers['publication']\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(df_all)} newspaper articles\")\n",
    "print(f\"Sample data columns: {df_all.columns.tolist()}\")\n",
    "\n",
    "# Examine the first rows to understand the data\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IPI (response variable) data - this is monthly data that we'll use for supervision\n",
    "df_ipi = pd.read_csv(\"../data/processed/ipi_data.csv\")\n",
    "\n",
    "# Display the IPI data\n",
    "print(f\"Loaded IPI data with {len(df_ipi)} records\")\n",
    "print(\"Columns in df_ipi:\", df_ipi.columns.tolist())\n",
    "df_ipi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd7de0",
   "metadata": {},
   "source": [
    "### Merge newspaper data with IPI data\n",
    "\n",
    "Since IPI data is monthly while newspaper text is at article level, we need to merge them to assign the monthly IPI values to each article in the corresponding month and publication. The newspaper data uses YYYY-MM-DD format for dates while the IPI data uses YYYY-MM format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1028e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create month_key in df_all to match df_ipi format\n",
    "# Remove time component if present and convert to month_key\n",
    "df_all['month_key'] = pd.to_datetime(df_all['date'].str.split().str[0]).dt.strftime('%Y-%m')\n",
    "\n",
    "# Merge the dataframes on month_key and publication\n",
    "df_merged = pd.merge(\n",
    "    df_all, \n",
    "    df_ipi[['month', 'publication', 'ipi_value']], \n",
    "left_on=['month_key', 'publication'],\n",
    "    right_on=['month', 'publication']\n",
    ")\n",
    "\n",
    "# Consolidate 'month' column:\n",
    "# If 'month_y' (from df_ipi) exists, rename it to 'month'.\n",
    "# If 'month_x' (from df_all) also exists, it's likely redundant with 'month_key' or not the IPI-aligned month, so drop it.\n",
    "if 'month_y' in df_merged.columns:\n",
    "    df_merged.rename(columns={'month_y': 'month'}, inplace=True)\n",
    "    if 'month_x' in df_merged.columns:\n",
    "        df_merged.drop(columns=['month_x'], inplace=True, errors='ignore')\n",
    "elif 'month' not in df_merged.columns:\n",
    "    # This case should ideally not happen if the merge is successful and df_ipi has 'month'\n",
    "    print(\"Warning: 'month' column is missing after merge and suffix handling.\")\n",
    "\n",
    "\n",
    "print(\"Columns in df_merged after merge and rename:\", df_merged.columns.tolist())\n",
    "\n",
    "print(f\"Merged data shape: {df_merged.shape}\")\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(df_merged[['date', 'publication', 'month_key', 'ipi_value']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b4748",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "Now we'll implement supervised LDA using tomotopy's SLDAModel, which directly incorporates the IPI values during topic inference. The `vars` parameter in SLDAModel specifies that we have one continuous response variable (the normalized IPI value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c734b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Step 1: Text preprocessing for tomotopy\n",
    "print(\"Preprocessing text data...\")\n",
    "\n",
    "# Check if 'article' or 'content' column exists\n",
    "text_column = 'article' if 'article' in df_merged.columns else 'content'\n",
    "\n",
    "# Define a list of stopwords (standard English stopwords + common newspaper words)\n",
    "stopwords = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "                 \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "                 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "                 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "                 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', \n",
    "                 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "                 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "                 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "                 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
    "                 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "                 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n",
    "                 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "                 \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                 'wouldn', \"wouldn't\", 'said', 'mr', 'also'])\n",
    "\n",
    "# Tokenization function for tomotopy\n",
    "def preprocess_text(text):\n",
    "    # Keep words with 3 or more alphabetic characters\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "# Apply preprocessing to get tokenized documents\n",
    "docs_tokens = df_merged[text_column].apply(preprocess_text).tolist()\n",
    "\n",
    "# Get the IPI values as supervision labels\n",
    "ipi_values = df_merged['ipi_value'].tolist()\n",
    "\n",
    "# Normalize IPI values to smaller range for better numerical stability\n",
    "# Scale to 0-1 range which works better with tomotopy\n",
    "ipi_min = min(ipi_values)\n",
    "ipi_max = max(ipi_values)\n",
    "ipi_values_norm = [(val - ipi_min) / (ipi_max - ipi_min) for val in ipi_values]\n",
    "print(f\"Normalized IPI values range: {min(ipi_values_norm)} to {max(ipi_values_norm)}\")\n",
    "\n",
    "# After tokenization: check how many documents have tokens\n",
    "num_docs_with_tokens = sum(1 for tokens in docs_tokens if tokens)\n",
    "print(f\"Documents with tokens: {num_docs_with_tokens} / {len(docs_tokens)}\")\n",
    "print(\"Sample tokenized docs:\", docs_tokens[:5])\n",
    "\n",
    "\n",
    "# Step 2: Initialize and train the supervised LDA model\n",
    "print(f\"Training sLDA model with {num_topics} topics...\")\n",
    "\n",
    "# Initialize the sLDA model with hyperparameters\n",
    "# For the 'vars' parameter in tomotopy:\n",
    "# - 1: Use the value 1 for continuous variables\n",
    "# - Integer > 1: Number of categories for categorical variables\n",
    "slda_model = tp.SLDAModel(\n",
    "    k=num_topics,  # Number of topics\n",
    "    alpha=0.1,     # Prior document-topic density\n",
    "    eta=0.01,      # Prior topic-word density\n",
    "    seed=42,       # For reproducibility\n",
    "    vars=['l']       # One continuous regression variable (the IPI value) - Reverted to 'l'\n",
    ")\n",
    "\n",
    "# Add documents with their corresponding labels (IPI values)\n",
    "for i, (tokens, ipi) in enumerate(zip(docs_tokens, ipi_values_norm)):\n",
    "    if tokens:  # Ensure document has tokens after preprocessing\n",
    "        slda_model.add_doc(tokens, [float(ipi)])  # tomotopy expects label as a list, ensure float type\n",
    "\n",
    "# After adding documents: check the number of documents in the model\n",
    "print(f\"Number of documents in sLDA model: {len(slda_model.docs)}\")\n",
    "assert len(slda_model.docs) > 0, \"No documents were added to the sLDA model!\"\n",
    "\n",
    "# Train the model\n",
    "print(\"Training sLDA model...\")\n",
    "for i in range(0, 100, 10):\n",
    "    slda_model.train(10)  # Train 10 iterations at a time\n",
    "    print(f'Iteration: {i+10}\\tLog-likelihood: {slda_model.ll_per_word}')\n",
    "\n",
    "print(\"sLDA model training complete\")\n",
    "\n",
    "# Evaluate the model's predictive performance - tomotopy will give us regression coefficients\n",
    "raw_coef_output = slda_model.get_regression_coef() # Expected: [[c0, c1, ..., c_N-1]] or [c0, ..., c_N-1]\n",
    "\n",
    "# Correctly extract 1D list of coefficients\n",
    "if isinstance(raw_coef_output, (list, np.ndarray)) and \\\n",
    "   len(raw_coef_output) == 1 and \\\n",
    "   isinstance(raw_coef_output[0], (list, np.ndarray)) and \\\n",
    "   all(isinstance(c, (int, float, np.number)) for c in raw_coef_output[0]):\n",
    "    # Handles [[c0, c1, ..., cN-1]]\n",
    "    coefficients_1d = [float(c) for c in raw_coef_output[0]]\n",
    "elif isinstance(raw_coef_output, (list, np.ndarray)) and \\\n",
    "     all(isinstance(c, (int, float, np.number)) for c in raw_coef_output):\n",
    "    # Handles [c0, c1, ..., cN-1] (already 1D)\n",
    "    coefficients_1d = [float(c) for c in raw_coef_output]\n",
    "else:\n",
    "    print(f\"Warning: Unexpected format for regression coefficients. Got: {raw_coef_output}\")\n",
    "    coefficients_1d = [] # Fallback\n",
    "\n",
    "# Define intercept and topic_coefs\n",
    "# Assuming if len(coefficients_1d) == num_topics, all are topic_coefs and intercept is 0\n",
    "# If len(coefficients_1d) == num_topics + 1, first is intercept, rest are topic_coefs\n",
    "if len(coefficients_1d) == num_topics:\n",
    "    intercept = 0.0\n",
    "    topic_coefs = coefficients_1d\n",
    "    print(f\"Interpreting {len(coefficients_1d)} coefficients as all topic-specific, intercept = 0.0\")\n",
    "elif len(coefficients_1d) == num_topics + 1:\n",
    "    intercept = coefficients_1d[0]\n",
    "    topic_coefs = coefficients_1d[1:]\n",
    "    print(f\"Interpreting {len(coefficients_1d)} coefficients as intercept + {len(topic_coefs)} topic-specific.\")\n",
    "else:\n",
    "    print(f\"Warning: Unexpected number of coefficients ({len(coefficients_1d)}). Expected {num_topics} or {num_topics + 1}. Defaulting intercept to 0 and using all as topic_coefs if available.\")\n",
    "    intercept = 0.0\n",
    "    topic_coefs = coefficients_1d if coefficients_1d else []\n",
    "\n",
    "\n",
    "print(\"\\nRegression coefficients (topic influence on IPI):\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Number of topic coefficients: {len(topic_coefs)} (should be {num_topics})\")\n",
    "if len(topic_coefs) != num_topics:\n",
    "    print(f\"WARNING: Number of topic coefficients ({len(topic_coefs)}) does not match num_topics ({num_topics})! Check model initialization and document addition.\")\n",
    "for k, coef in enumerate(topic_coefs):\n",
    "    print(f\"Topic {k}: {coef:.4f}\")\n",
    "\n",
    "\n",
    "# Get topic distributions for validation set to evaluate model performance\n",
    "# For sLDA, we need to calculate predictions and compare with actual values\n",
    "print(\"\\nCalculating model predictions...\")\n",
    "\n",
    "# Get regression coefficients from sLDA model\n",
    "raw_coef_output_pred = slda_model.get_regression_coef() # Renamed to avoid conflict if any subtle scope issue\n",
    "\n",
    "# Correctly extract 1D list of coefficients for prediction\n",
    "if isinstance(raw_coef_output_pred, (list, np.ndarray)) and \\\n",
    "   len(raw_coef_output_pred) == 1 and \\\n",
    "   isinstance(raw_coef_output_pred[0], (list, np.ndarray)) and \\\n",
    "   all(isinstance(c, (int, float, np.number)) for c in raw_coef_output_pred[0]):\n",
    "    # Handles [[c0, c1, ..., cN-1]]\n",
    "    coefficients_1d_pred = [float(c) for c in raw_coef_output_pred[0]]\n",
    "elif isinstance(raw_coef_output_pred, (list, np.ndarray)) and \\\n",
    "     all(isinstance(c, (int, float, np.number)) for c in raw_coef_output_pred):\n",
    "    # Handles [c0, c1, ..., cN-1] (already 1D)\n",
    "    coefficients_1d_pred = [float(c) for c in raw_coef_output_pred]\n",
    "else:\n",
    "    print(f\"Warning: Unexpected format for regression coefficients during prediction. Got: {raw_coef_output_pred}\")\n",
    "    coefficients_1d_pred = [] # Fallback\n",
    "\n",
    "# Define intercept and topic_coefs for prediction (mirroring the logic above)\n",
    "if len(coefficients_1d_pred) == num_topics:\n",
    "    intercept_pred = 0.0\n",
    "    topic_coefs_pred = coefficients_1d_pred\n",
    "elif len(coefficients_1d_pred) == num_topics + 1:\n",
    "    intercept_pred = coefficients_1d_pred[0]\n",
    "    topic_coefs_pred = coefficients_1d_pred[1:]\n",
    "else: # Fallback, should match the logic for non-pred variables\n",
    "    intercept_pred = 0.0\n",
    "    topic_coefs_pred = coefficients_1d_pred if coefficients_1d_pred else []\n",
    "\n",
    "# Ensure intercept and topic_coefs used in this prediction block are the _pred versions\n",
    "intercept = intercept_pred\n",
    "topic_coefs = topic_coefs_pred\n",
    "\n",
    "predicted_values = []\n",
    "actual_values = []\n",
    "\n",
    "for i, doc in enumerate(slda_model.docs):\n",
    "    topic_dist = doc.get_topic_dist()\n",
    "    # Ensure topic_dist is 1D and matches topic_coefs\n",
    "    pred = intercept + sum(coef * prob for coef, prob in zip(topic_coefs, topic_dist))\n",
    "    predicted_values.append(float(pred))\n",
    "    actual = float(ipi_values_norm[i])  # Already normalized above\n",
    "    actual_values.append(actual)\n",
    "\n",
    "# Convert to 1D numpy arrays for sklearn metrics\n",
    "#import numpy as np\n",
    "predicted_values = np.array(predicted_values).flatten()\n",
    "actual_values = np.array(actual_values).flatten()\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "r2 = r2_score(actual_values, predicted_values)\n",
    "\n",
    "print(f\"Model evaluation: R² = {r2:.4f}, MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After tokenization\n",
    "num_docs_with_tokens = sum(1 for tokens in docs_tokens if tokens)\n",
    "print(f\"Documents with tokens: {num_docs_with_tokens} / {len(docs_tokens)}\")\n",
    "print(\"Sample tokenized docs:\", docs_tokens[:5])\n",
    "\n",
    "# After adding documents\n",
    "print(f\"Number of documents in sLDA model: {len(slda_model.docs)}\")\n",
    "assert len(slda_model.docs) > 0, \"No documents were added to the sLDA model!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831166ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the topics and regression coefficients\n",
    "# Extract the top words for each topic\n",
    "topic_terms = {}\n",
    "for topic_idx in range(num_topics):\n",
    "    # Get top 20 words for this topic\n",
    "    top_terms = slda_model.get_topic_words(topic_idx, top_n=20)\n",
    "    topic_weight = topic_coefs[topic_idx]  # Get regression coefficient\n",
    "    topic_terms[topic_idx] = (top_terms, topic_weight)\n",
    "\n",
    "# Print out the topics and their regression coefficients\n",
    "print(\"\\nTopic words and regression coefficients (topic influence on IPI):\")\n",
    "for topic_idx, (terms, weight) in topic_terms.items():\n",
    "    print(f\"\\nTopic {topic_idx}:\")\n",
    "    print(f\"Regression coefficient: {weight:.4f}\")\n",
    "    print(f\"Top terms: {', '.join([term for term, _ in terms[:10]])}\")\n",
    "    \n",
    "# Print topic weights for predicting IPI\n",
    "print(\"\\nRegression coefficients (topic influence on IPI):\")\n",
    "for k, coef in enumerate(topic_coefs):\n",
    "    print(f\"Topic {k}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bb1da",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top topics and their relationship to the IPI values\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    # Get topic words and their weights\n",
    "    if idx < num_topics:  # Make sure we don't exceed the number of topics\n",
    "        # Get the top 50 words for this topic\n",
    "        top_words = slda_model.get_topic_words(idx, top_n=50)\n",
    "        \n",
    "        # Create dictionary for the wordcloud\n",
    "        topic_dict = {word: weight for word, weight in top_words}\n",
    "        \n",
    "        # Create wordcloud\n",
    "        wc = WordCloud(width=500, height=300, background_color='white', max_words=50)\n",
    "        wc.generate_from_frequencies(topic_dict)\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        \n",
    "        # Add title with regression coefficient to show topic-IPI relationship\n",
    "        coef = topic_coefs[idx]\n",
    "        coef_sign = \"+\" if coef > 0 else \"\"\n",
    "        ax.set_title(f'Topic {idx + 1} (IPI Effect: {coef_sign}{coef:.3f})', fontsize=14)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')  # Turn off extra subplots if any\n",
    "\n",
    "plt.suptitle('Supervised LDA Topics – Word Clouds with IPI Effect', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"../models/topic_model\", exist_ok=True)\n",
    "\n",
    "# Save the sLDA model using tomotopy's native save method\n",
    "slda_model.save(\"../models/topic_model/slda_model\")\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec527a",
   "metadata": {},
   "source": [
    "## Topic Extraction & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract document-topic distributions for each article\n",
    "doc_topic_dists = []\n",
    "for doc in slda_model.docs:\n",
    "    doc_topic_dists.append(doc.get_topic_dist())\n",
    "\n",
    "# Create topic column names\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "\n",
    "# Create a DataFrame with topic distributions\n",
    "df_topics = pd.DataFrame(doc_topic_dists, columns=topic_cols)\n",
    "\n",
    "# Combine with metadata\n",
    "df_meta = df_merged[['date', 'publication', 'month']].reset_index(drop=True).copy()  # Using existing columns\n",
    "# Only keep rows for which we have topic distributions (some might have been filtered out)\n",
    "df_meta = df_meta.iloc[:len(doc_topic_dists)].copy()\n",
    "\n",
    "df_combined = pd.concat([df_meta, df_topics], axis=1)\n",
    "\n",
    "# Aggregate topic shares by month and publisher\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# Save to CSV with tomotopy_sLDA in the filename\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_sLDA.csv', index=False)\n",
    "\n",
    "print(\"✅ Saved: 'monthly_topic_shares_by_publisher_sLDA.csv'\")\n",
    "\n",
    "# Display the head of the saved CSV file\n",
    "df_check_csv = pd.read_csv('../data/processed/monthly_topic_shares_by_publisher_sLDA.csv')\n",
    "print(\"Head of 'monthly_topic_shares_by_publisher_sLDA.csv':\")\n",
    "df_check_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d739732",
   "metadata": {},
   "source": [
    "### Compare topic trends with IPI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the relationship between topic prevalence and IPI values\n",
    "\n",
    "# Get average IPI by month\n",
    "monthly_ipi = df_ipi.groupby('month')['ipi_value'].mean().reset_index()\n",
    "\n",
    "# Get average topic distributions by month (across all publishers)\n",
    "monthly_topics = df_monthly_pub.groupby('month')[topic_cols].mean().reset_index()\n",
    "\n",
    "# Merge the data\n",
    "df_trends = pd.merge(monthly_topics, monthly_ipi, on='month', how='inner')\n",
    "\n",
    "# Plot the trends of the top 3 most influential topics (based on regression coefficients)\n",
    "topic_importance = [(i, abs(c)) for i, c in enumerate(topic_coefs)]\n",
    "top_topics = sorted(topic_importance, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot IPI values\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "months = df_trends['month'].astype(str).tolist()\n",
    "ax1.plot(months, df_trends['ipi_value'], 'k-', label='IPI Value')\n",
    "\n",
    "# Plot topic trends\n",
    "for topic_idx, importance in top_topics:\n",
    "    topic_col = f'topic_{topic_idx}'\n",
    "    coef = topic_coefs[topic_idx]\n",
    "    coef_sign = \"+\" if coef > 0 else \"\"\n",
    "    ax2.plot(months, df_trends[topic_col], '--', \n",
    "            label=f'Topic {topic_idx} (IPI Effect: {coef_sign}{coef:.3f})')\n",
    "\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('IPI Value')\n",
    "ax2.set_ylabel('Topic Prevalence')\n",
    "ax1.set_xticklabels(months, rotation=90)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('IPI Values and Top Topic Trends Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0cb1",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The output CSV file (`monthly_topic_shares_by_publisher_sLDA.csv`) contains the monthly topic distributions for each publisher, which can be used for further analysis or visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
