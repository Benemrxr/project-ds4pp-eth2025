{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04b394e",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Outline:\n",
    "- 1) LDA (full sample)\n",
    "- 2) BERTopic \n",
    "\n",
    "To be further developed... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6eb94",
   "metadata": {},
   "source": [
    "## 1) LDA (full sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load publishers data from the corresponding file\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb24ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# compute topic modeling for all publishers (append all the samples)\n",
    "df_all = pd.concat([pd.read_csv(f\"../data/processed/newspapers/sample_{re.sub(r'\\\\W+','_ ', pub.lower()).strip('_')}.csv\") for pub in publishers['publication']], ignore_index=True)\n",
    "\n",
    "# Tokenization and stopword removal using regex and sklearn stopwords\n",
    "custom_stopwords = ENGLISH_STOP_WORDS.union({'said', 'mr', 'also'})\n",
    "\n",
    "def tokenize_and_clean(text):\n",
    "    # Keep words with 3 or more alphabetic characters\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return [t for t in tokens if t not in custom_stopwords]\n",
    "\n",
    "df_all['tokens'] = df_all['article'].apply(tokenize_and_clean)\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df_all['tokens'])\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(text) for text in df_all['tokens']]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 10\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Plot wordclouds\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    topic_words = dict(lda_model.show_topic(idx, 50))\n",
    "    wc = WordCloud(width=500, height=300, background_color='white', max_words=50)\n",
    "    wc.generate_from_frequencies(topic_words)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.set_title(f'Topic {idx + 1}', fontsize=14)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('LDA Topics â€“ Word Clouds', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b009055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LDA model and dictionary\n",
    "lda_model.save(\"../models/topic_model/lda_model.gensim\")\n",
    "dictionary.save(\"../models/topic_model/lda_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Load model and dictionary\n",
    "lda_model = models.LdaModel.load(\"../models/topic_model/lda_model.gensim\")\n",
    "dictionary = corpora.Dictionary.load(\"../models/topic_model/lda_dictionary.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb79eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df_all['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f'Coherence Score (c_v): {coherence_score:.4f}')\n",
    "# A score above 0.4 is generally considered good for topic coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# --- Assumes these exist ---\n",
    "# - df['tokens'] = list of preprocessed word tokens\n",
    "# - df['date'] = parsed datetime\n",
    "# - df['publication'] = publisher name\n",
    "# - lda_model = trained gensim LdaModel\n",
    "# - dictionary = gensim Dictionary used to train the model\n",
    "\n",
    "# Load all samples from new processed newspapers folder\n",
    "df_all = pd.concat([pd.read_csv(f\"../data/processed/newspapers/sample_{re.sub(r'\\\\W+','_ ', pub.lower()).strip('_')}.csv\") for pub in publishers['publication']], ignore_index=True)\n",
    "\n",
    "# Step 1: Convert tokens to bag-of-words\n",
    "corpus = [dictionary.doc2bow(text) for text in df_all['tokens']]\n",
    "\n",
    "# Step 2: Get topic distribution for each article\n",
    "def get_topic_dist(bow):\n",
    "    # Return full-length vector with zero entries where necessary\n",
    "    dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    return [prob for _, prob in dist]\n",
    "\n",
    "df_all['topic_distribution'] = [get_topic_dist(doc) for doc in corpus]\n",
    "\n",
    "# Step 3: Unpack topic distributions into separate columns\n",
    "num_topics = lda_model.num_topics\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topics = pd.DataFrame(df_all['topic_distribution'].tolist(), columns=topic_cols)\n",
    "\n",
    "# Step 4: Combine with metadata\n",
    "df_meta = df_all[['date', 'publication']].copy()\n",
    "df_combined = pd.concat([df_meta, df_topics], axis=1)\n",
    "df_combined['month'] = pd.to_datetime(df_combined['date'], format='mixed', errors='coerce').dt.to_period('M')\n",
    "\n",
    "# Step 5: Aggregate topic shares by month and publisher\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# Step 6: Save to CSV\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher.csv', index=False)\n",
    "\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6286b3f-4138-48ec-8dfa-94a8930a2318",
   "metadata": {},
   "source": [
    "## 2) Supervised feature engineering\n",
    "\n",
    "Goal: Pre-define topics relevant to our context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9414a1-9850-4622-a6f1-e3b703219783",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/newspapers/sample_reuters.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, pub\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate all article samples\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43m[\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/newspapers/sample_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msafe_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpub\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpub\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpublishers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublication\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m]\u001b[49m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 2. Define keyword filter ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m topic_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupply chain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, pub\u001b[38;5;241m.\u001b[39mlower())\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Concatenate all article samples\u001b[39;00m\n\u001b[0;32m     17\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/newspapers/sample_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msafe_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpub\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m publishers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m ], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# --- 2. Define keyword filter ---\u001b[39;00m\n\u001b[0;32m     23\u001b[0m topic_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupply chain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/newspapers/sample_reuters.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# --- 1. Load publisher article data ---\n",
    "# Load publishers\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "\n",
    "# Define safe filename generator\n",
    "def safe_filename(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "# Concatenate all article samples\n",
    "df_all = pd.concat([\n",
    "    pd.read_csv(f\"../data/processed/newspapers/sample_{safe_filename(pub)}.csv\")\n",
    "    for pub in publishers['publication']\n",
    "], ignore_index=True)\n",
    "\n",
    "# --- 2. Define keyword filter ---\n",
    "topic_keywords = [\n",
    "    \"manufacturing\", \"factory\", \"production\", \"industry\", \"output\",\n",
    "    \"supply chain\", \"logistics\", \"transport\", \"shortage\"\n",
    "]\n",
    "\n",
    "# --- 3. Clean and filter text ---\n",
    "def clean_text(text_input):\n",
    "    text_input = str(text_input).lower()\n",
    "    text_input = re.sub(r'\\d+', '', text_input)\n",
    "    text_input = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text_input)\n",
    "    text_input = re.sub(r'\\s+', ' ', text_input).strip()\n",
    "    return text_input\n",
    "\n",
    "def filter_keywords(text, keywords):\n",
    "    text = text.lower()\n",
    "    return ' '.join([\n",
    "        word for word in text.split()\n",
    "        if any(k in word for k in keywords)\n",
    "    ])\n",
    "\n",
    "# --- 4. Aggregate articles per month ---\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "df_monthly = df_all.groupby('month')['article'].apply(lambda x: ' '.join(x.dropna())).reset_index()\n",
    "\n",
    "# --- 5. Apply keyword filtering ---\n",
    "df_monthly['filtered_content'] = df_monthly['article'].apply(lambda x: filter_keywords(x, topic_keywords))\n",
    "\n",
    "# --- 6. Load IPI data and merge ---\n",
    "df_indpro = pd.read_csv('https://fred.stlouisfed.org/graph/fredgraph.csv?id=INDPRO', parse_dates=['observation_date'])\n",
    "df_indpro.rename(columns={'observation_date': 'date', 'INDPRO': 'ipi'}, inplace=True)\n",
    "df_indpro['month'] = df_indpro['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "df_keywords_monthly = pd.merge(df_monthly, df_indpro[['month', 'ipi']], on='month', how='inner')\n",
    "\n",
    "# --- 7. TF-IDF Vectorization ---\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=10,\n",
    "    stop_words='english',\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "    preprocessor=clean_text\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df_keywords_monthly['filtered_content'].fillna(''))\n",
    "\n",
    "df_features = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=[f\"kw_{word}\" for word in vectorizer.get_feature_names_out()]\n",
    ")\n",
    "df_features['month'] = df_keywords_monthly['month'].values\n",
    "df_features['ipi'] = df_keywords_monthly['ipi'].values\n",
    "\n",
    "# --- 8. Save final DataFrame ---\n",
    "df_features.to_csv(\"../data/processed/df_industry_keywords_monthly.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52ae05",
   "metadata": {},
   "source": [
    "## 3) BERTopic\n",
    "\n",
    "Goal: Topic shares by month-publisher using BERTopic for the topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73ad9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:08<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from difflib import get_close_matches\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load publishers\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "\n",
    "# Get all sample files\n",
    "data_dir = \"../data/processed/newspapers/\"\n",
    "available_files = os.listdir(data_dir)\n",
    "\n",
    "# Extract clean basenames\n",
    "available_basenames = {\n",
    "    re.sub(r'^sample_|\\.csv$', '', fname): fname\n",
    "    for fname in available_files\n",
    "    if fname.startswith(\"sample_\") and fname.endswith(\".csv\")\n",
    "}\n",
    "\n",
    "# Helper to sanitize\n",
    "def sanitize(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "# Load and concat all files\n",
    "dfs = []\n",
    "for pub in tqdm(publishers['publication'], desc=\"Loading data\"):\n",
    "    pub_clean = sanitize(pub)\n",
    "    match = get_close_matches(pub_clean, available_basenames.keys(), n=1, cutoff=0.7)\n",
    "    if match:\n",
    "        matched_filename = os.path.join(data_dir, available_basenames[match[0]])\n",
    "        dfs.append(pd.read_csv(matched_filename))\n",
    "    else:\n",
    "        print(f\"No file found for: {pub}\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Clean articles\n",
    "def clean_text(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_all['clean_article'] = df_all['article'].astype(str).apply(clean_text)\n",
    "df_all = df_all[df_all['clean_article'].str.strip().astype(bool)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94297f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17d9d9a33574e738473f985d162b56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 21:38:29,745 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Fitting BERTopic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2025-06-09 21:42:14,514 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-06-09 21:42:14,519 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-09 21:42:14,720 - BERTopic - Cluster - Completed âœ“\n",
      "2025-06-09 21:42:14,751 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-09 21:43:00,574 - BERTopic - Representation - Completed âœ“\n",
      "2025-06-09 21:43:01,608 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# ðŸ§¹ Text data\n",
    "texts = df_all['clean_article'].tolist()\n",
    "\n",
    "# âš¡ Embedding model\n",
    "embedding_model = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# UMAP and KMeans\n",
    "umap_model = UMAP(n_components=5, random_state=42)\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=10, random_state=42) # choose number of clusters based for easier comparison with LDA\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=kmeans_model,\n",
    "    embedding_model=None,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print(\"Fitting BERTopic...\")\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# Save results\n",
    "topic_model.save(\"../models/topic_model/bertopic_model\")\n",
    "np.save(\"../models/topic_model/bertopic_embeddings.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72fb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: 'monthly_topic_shares_by_publisher_bertopic.csv'\n"
     ]
    }
   ],
   "source": [
    "# Get number of real topics\n",
    "topic_ids = sorted([t for t in topic_model.get_topics().keys() if t != -1])\n",
    "topic_cols = [f\"topic_{i}\" for i in topic_ids]\n",
    "\n",
    "df_meta = df_all[['date', 'publication']].reset_index(drop=True)\n",
    "df_combined = df_meta.copy()\n",
    "df_combined['topic'] = topics\n",
    "\n",
    "# Extract month\n",
    "df_combined['month'] = pd.to_datetime(df_combined['date'], format='mixed', errors='coerce').dt.to_period('M')\n",
    "\n",
    "# One-hot encode topics\n",
    "df_onehot = pd.get_dummies(df_combined['topic'], prefix='topic')\n",
    "\n",
    "# Combine with meta\n",
    "df_combined = pd.concat([df_combined[['month', 'publication']], df_onehot], axis=1)\n",
    "\n",
    "# Group by month + publisher and average (which is topic share)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication']).mean().reset_index()\n",
    "\n",
    "# Save result\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_bertopic.csv', index=False)\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_bertopic.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaaf743",
   "metadata": {},
   "source": [
    "## 4) Mini-LLM Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5eee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b75bbd15aa14e89942ab3809f72250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: 'monthly_topic_shares_by_publisher_minillm.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Clean text ---\n",
    "def tokenize_and_clean(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    custom_stopwords = {'said', 'mr', 'also'}  # add more if needed\n",
    "    return ' '.join([t for t in tokens if t not in custom_stopwords])\n",
    "\n",
    "df_all['clean_text'] = df_all['article'].apply(tokenize_and_clean)\n",
    "\n",
    "# --- Step 2: Generate embeddings ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df_all['clean_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# --- Step 3: Cluster into topics ---\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "df_all['topic_cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# --- Step 4: Topic distribution per document ---\n",
    "similarities = cosine_similarity(embeddings, kmeans.cluster_centers_)\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topic_dist = pd.DataFrame(similarities, columns=topic_cols)\n",
    "df_all = pd.concat([df_all, df_topic_dist], axis=1)\n",
    "\n",
    "# --- Step 5: Aggregate over time and publisher ---\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M')\n",
    "\n",
    "df_combined = pd.concat([df_all[['month', 'publication']], df_all[topic_cols]], axis=1)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# --- Step 6: Save ---\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_minillm.csv', index=False)\n",
    "\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_minillm.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c6bebf",
   "metadata": {},
   "source": [
    "## 5) LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a460a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:09<00:00,  2.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4cb2764385473781efcedbde49496b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440479f3ea9947f5b3b053f1b5a59b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36501fec30eb42a39b76c17229d6395e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c67f422b1ea42d9b0f732bdacc9255d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1ab66dfe244b6c951d95dd33ddf251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d2df60768448ba9042e0d9f768737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f073b763948e7a4838e40ed759fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "  0%|          | 0/254633 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 2/254633 [00:15<561:56:27,  7.94s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 3/254633 [00:21<479:15:36,  6.78s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 4/254633 [00:24<382:03:55,  5.40s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 5/254633 [00:25<294:27:38,  4.16s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 6/254633 [00:28<254:31:46,  3.60s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 7/254633 [00:35<327:37:57,  4.63s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 8/254633 [00:39<313:26:20,  4.43s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 9/254633 [00:45<360:57:57,  5.10s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 10/254633 [00:48<306:14:52,  4.33s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 11/254633 [00:53<328:11:26,  4.64s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 12/254633 [00:55<268:29:36,  3.80s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  0%|          | 13/254633 [01:01<322:36:31,  4.56s/it]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- Step 1: Load and clean data ---\n",
    "\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "data_dir = \"../data/processed/newspapers/\"\n",
    "available_files = os.listdir(data_dir)\n",
    "\n",
    "available_basenames = {\n",
    "    re.sub(r'^sample_|\\.csv$', '', fname): fname\n",
    "    for fname in available_files\n",
    "    if fname.startswith(\"sample_\") and fname.endswith(\".csv\")\n",
    "}\n",
    "\n",
    "def sanitize(pub):\n",
    "    return re.sub(r'\\W+', '_', pub.lower()).strip('_')\n",
    "\n",
    "dfs = []\n",
    "for pub in tqdm(publishers['publication'], desc=\"Loading data\"):\n",
    "    pub_clean = sanitize(pub)\n",
    "    match = get_close_matches(pub_clean, available_basenames.keys(), n=1, cutoff=0.7)\n",
    "    if match:\n",
    "        dfs.append(pd.read_csv(os.path.join(data_dir, available_basenames[match[0]])))\n",
    "    else:\n",
    "        print(f\"No file found for: {pub}\")\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# --- Step 2: Clean text ---\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    custom_stopwords = {'said', 'mr', 'also'}\n",
    "    return ' '.join([t for t in tokens if t not in custom_stopwords])\n",
    "\n",
    "df_all['clean_text'] = df_all['article'].astype(str).apply(clean_text)\n",
    "df_all = df_all[df_all['clean_text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "# --- Step 3: Setup TinyLlama model and pipeline ---\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\").to(device)\n",
    "\n",
    "topic_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device != 'cpu' else -1\n",
    ")\n",
    "\n",
    "def generate_topics(text):\n",
    "    prompt = f\"Extract 10 relevant topics or keywords from the following text:\\n\\n{text}\\n\\nTopics:\"\n",
    "    output = topic_pipe(prompt, truncation=True, max_length=512, do_sample=False)\n",
    "    response = output[0]['generated_text']\n",
    "    topics = response.split(\"Topics:\")[-1].strip()\n",
    "    return topics\n",
    "\n",
    "tqdm.pandas()\n",
    "df_all['topics'] = df_all['clean_text'].progress_apply(generate_topics)\n",
    "\n",
    "# --- Step 4: Embedding and clustering ---\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode(df_all['topics'].tolist(), show_progress_bar=True)\n",
    "\n",
    "num_topics = 10\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "df_all['topic_cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# --- Step 5: Topic distribution per document ---\n",
    "\n",
    "similarities = cosine_similarity(embeddings, kmeans.cluster_centers_)\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "df_topic_dist = pd.DataFrame(similarities, columns=topic_cols)\n",
    "df_all = pd.concat([df_all, df_topic_dist], axis=1)\n",
    "\n",
    "# --- Step 6: Aggregate over time and publisher ---\n",
    "\n",
    "df_all['date'] = pd.to_datetime(df_all['date'], errors='coerce')\n",
    "df_all['month'] = df_all['date'].dt.to_period('M')\n",
    "\n",
    "df_combined = pd.concat([df_all[['month', 'publication']], df_all[topic_cols]], axis=1)\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# --- Step 7: Save ---\n",
    "\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_tinyllama.csv', index=False)\n",
    "print(\"Saved: 'monthly_topic_shares_by_publisher_tinyllama.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a2d6e",
   "metadata": {},
   "source": [
    "## 6) Supervised LDA (sLDA) Implementation with Tomotopy\n",
    "\n",
    "Unlike the previous approach that used a two-stage method (sklearn LDA + Linear Regression), we'll implement true supervised LDA using tomotopy, which integrates the supervision signal (IPI data) directly into the topic modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tomotopy as tp  # We're using tomotopy for supervised LDA\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3068f",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's load the pre-processed newspaper data and the IPI (supervision) data for sLDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ca27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load publishers information\n",
    "publishers = pd.read_csv(\"../data/processed/publishers.csv\")\n",
    "print(f\"Loaded information for {len(publishers)} publishers\")\n",
    "\n",
    "# Load all samples from processed newspapers folder\n",
    "# Create a function to convert publisher names to filenames\n",
    "def publisher_to_filename(publisher_name):\n",
    "    # Replace special characters with underscores\n",
    "    return f\"sample_{re.sub(r'\\W+', '_', publisher_name.lower()).strip('_')}.csv\"\n",
    "\n",
    "# Load all sample files\n",
    "df_all = pd.concat([\n",
    "    pd.read_csv(f\"../data/processed/newspapers/{publisher_to_filename(pub)}\")\n",
    "    for pub in publishers['publication']\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(df_all)} newspaper articles\")\n",
    "print(f\"Sample data columns: {df_all.columns.tolist()}\")\n",
    "\n",
    "# Examine the first rows to understand the data\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IPI (response variable) data - this is monthly data that we'll use for supervision\n",
    "df_ipi = pd.read_csv(\"../data/processed/ipi_data.csv\")\n",
    "\n",
    "# Display the IPI data\n",
    "print(f\"Loaded IPI data with {len(df_ipi)} records\")\n",
    "print(\"Columns in df_ipi:\", df_ipi.columns.tolist())\n",
    "df_ipi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd7de0",
   "metadata": {},
   "source": [
    "### Merge newspaper data with IPI data\n",
    "\n",
    "Since IPI data is monthly while newspaper text is at article level, we need to merge them to assign the monthly IPI values to each article in the corresponding month and publication. The newspaper data uses YYYY-MM-DD format for dates while the IPI data uses YYYY-MM format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1028e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create month_key in df_all to match df_ipi format\n",
    "# Remove time component if present and convert to month_key\n",
    "df_all['month_key'] = pd.to_datetime(df_all['date'].str.split().str[0]).dt.strftime('%Y-%m')\n",
    "\n",
    "# Merge the dataframes on month_key and publication\n",
    "df_merged = pd.merge(\n",
    "    df_all, \n",
    "    df_ipi[['month', 'publication', 'ipi_value']], \n",
    "left_on=['month_key', 'publication'],\n",
    "    right_on=['month', 'publication']\n",
    ")\n",
    "\n",
    "# Consolidate 'month' column:\n",
    "# If 'month_y' (from df_ipi) exists, rename it to 'month'.\n",
    "# If 'month_x' (from df_all) also exists, it's likely redundant with 'month_key' or not the IPI-aligned month, so drop it.\n",
    "if 'month_y' in df_merged.columns:\n",
    "    df_merged.rename(columns={'month_y': 'month'}, inplace=True)\n",
    "    if 'month_x' in df_merged.columns:\n",
    "        df_merged.drop(columns=['month_x'], inplace=True, errors='ignore')\n",
    "elif 'month' not in df_merged.columns:\n",
    "    # This case should ideally not happen if the merge is successful and df_ipi has 'month'\n",
    "    print(\"Warning: 'month' column is missing after merge and suffix handling.\")\n",
    "\n",
    "\n",
    "print(\"Columns in df_merged after merge and rename:\", df_merged.columns.tolist())\n",
    "\n",
    "print(f\"Merged data shape: {df_merged.shape}\")\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(df_merged[['date', 'publication', 'month_key', 'ipi_value']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b4748",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "\n",
    "Now we'll implement supervised LDA using tomotopy's SLDAModel, which directly incorporates the IPI values during topic inference. The `vars` parameter in SLDAModel specifies that we have one continuous response variable (the normalized IPI value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c734b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Step 1: Text preprocessing for tomotopy\n",
    "print(\"Preprocessing text data...\")\n",
    "\n",
    "# Check if 'article' or 'content' column exists\n",
    "text_column = 'article' if 'article' in df_merged.columns else 'content'\n",
    "\n",
    "# Define a list of stopwords (standard English stopwords + common newspaper words)\n",
    "stopwords = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "                 \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "                 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "                 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "                 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', \n",
    "                 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "                 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "                 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "                 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n",
    "                 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
    "                 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "                 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n",
    "                 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "                 \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "                 'wouldn', \"wouldn't\", 'said', 'mr', 'also'])\n",
    "\n",
    "# Tokenization function for tomotopy\n",
    "def preprocess_text(text):\n",
    "    # Keep words with 3 or more alphabetic characters\n",
    "    tokens = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "# Apply preprocessing to get tokenized documents\n",
    "docs_tokens = df_merged[text_column].apply(preprocess_text).tolist()\n",
    "\n",
    "# Get the IPI values as supervision labels\n",
    "ipi_values = df_merged['ipi_value'].tolist()\n",
    "\n",
    "# Normalize IPI values to smaller range for better numerical stability\n",
    "# Scale to 0-1 range which works better with tomotopy\n",
    "ipi_min = min(ipi_values)\n",
    "ipi_max = max(ipi_values)\n",
    "ipi_values_norm = [(val - ipi_min) / (ipi_max - ipi_min) for val in ipi_values]\n",
    "print(f\"Normalized IPI values range: {min(ipi_values_norm)} to {max(ipi_values_norm)}\")\n",
    "\n",
    "# After tokenization: check how many documents have tokens\n",
    "num_docs_with_tokens = sum(1 for tokens in docs_tokens if tokens)\n",
    "print(f\"Documents with tokens: {num_docs_with_tokens} / {len(docs_tokens)}\")\n",
    "print(\"Sample tokenized docs:\", docs_tokens[:5])\n",
    "\n",
    "\n",
    "# Step 2: Initialize and train the supervised LDA model\n",
    "print(f\"Training sLDA model with {num_topics} topics...\")\n",
    "\n",
    "# Initialize the sLDA model with hyperparameters\n",
    "# For the 'vars' parameter in tomotopy:\n",
    "# - 1: Use the value 1 for continuous variables\n",
    "# - Integer > 1: Number of categories for categorical variables\n",
    "slda_model = tp.SLDAModel(\n",
    "    k=num_topics,  # Number of topics\n",
    "    alpha=0.1,     # Prior document-topic density\n",
    "    eta=0.01,      # Prior topic-word density\n",
    "    seed=42,       # For reproducibility\n",
    "    vars=['l']       # One continuous regression variable (the IPI value) - Reverted to 'l'\n",
    ")\n",
    "\n",
    "# Add documents with their corresponding labels (IPI values)\n",
    "for i, (tokens, ipi) in enumerate(zip(docs_tokens, ipi_values_norm)):\n",
    "    if tokens:  # Ensure document has tokens after preprocessing\n",
    "        slda_model.add_doc(tokens, [float(ipi)])  # tomotopy expects label as a list, ensure float type\n",
    "\n",
    "# After adding documents: check the number of documents in the model\n",
    "print(f\"Number of documents in sLDA model: {len(slda_model.docs)}\")\n",
    "assert len(slda_model.docs) > 0, \"No documents were added to the sLDA model!\"\n",
    "\n",
    "# Train the model\n",
    "print(\"Training sLDA model...\")\n",
    "for i in range(0, 100, 10):\n",
    "    slda_model.train(10)  # Train 10 iterations at a time\n",
    "    print(f'Iteration: {i+10}\\tLog-likelihood: {slda_model.ll_per_word}')\n",
    "\n",
    "print(\"sLDA model training complete\")\n",
    "\n",
    "# Evaluate the model's predictive performance - tomotopy will give us regression coefficients\n",
    "raw_coef_output = slda_model.get_regression_coef() # Expected: [[c0, c1, ..., c_N-1]] or [c0, ..., c_N-1]\n",
    "\n",
    "# Correctly extract 1D list of coefficients\n",
    "if isinstance(raw_coef_output, (list, np.ndarray)) and \\\n",
    "   len(raw_coef_output) == 1 and \\\n",
    "   isinstance(raw_coef_output[0], (list, np.ndarray)) and \\\n",
    "   all(isinstance(c, (int, float, np.number)) for c in raw_coef_output[0]):\n",
    "    # Handles [[c0, c1, ..., cN-1]]\n",
    "    coefficients_1d = [float(c) for c in raw_coef_output[0]]\n",
    "elif isinstance(raw_coef_output, (list, np.ndarray)) and \\\n",
    "     all(isinstance(c, (int, float, np.number)) for c in raw_coef_output):\n",
    "    # Handles [c0, c1, ..., cN-1] (already 1D)\n",
    "    coefficients_1d = [float(c) for c in raw_coef_output]\n",
    "else:\n",
    "    print(f\"Warning: Unexpected format for regression coefficients. Got: {raw_coef_output}\")\n",
    "    coefficients_1d = [] # Fallback\n",
    "\n",
    "# Define intercept and topic_coefs\n",
    "# Assuming if len(coefficients_1d) == num_topics, all are topic_coefs and intercept is 0\n",
    "# If len(coefficients_1d) == num_topics + 1, first is intercept, rest are topic_coefs\n",
    "if len(coefficients_1d) == num_topics:\n",
    "    intercept = 0.0\n",
    "    topic_coefs = coefficients_1d\n",
    "    print(f\"Interpreting {len(coefficients_1d)} coefficients as all topic-specific, intercept = 0.0\")\n",
    "elif len(coefficients_1d) == num_topics + 1:\n",
    "    intercept = coefficients_1d[0]\n",
    "    topic_coefs = coefficients_1d[1:]\n",
    "    print(f\"Interpreting {len(coefficients_1d)} coefficients as intercept + {len(topic_coefs)} topic-specific.\")\n",
    "else:\n",
    "    print(f\"Warning: Unexpected number of coefficients ({len(coefficients_1d)}). Expected {num_topics} or {num_topics + 1}. Defaulting intercept to 0 and using all as topic_coefs if available.\")\n",
    "    intercept = 0.0\n",
    "    topic_coefs = coefficients_1d if coefficients_1d else []\n",
    "\n",
    "\n",
    "print(\"\\nRegression coefficients (topic influence on IPI):\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"Number of topic coefficients: {len(topic_coefs)} (should be {num_topics})\")\n",
    "if len(topic_coefs) != num_topics:\n",
    "    print(f\"WARNING: Number of topic coefficients ({len(topic_coefs)}) does not match num_topics ({num_topics})! Check model initialization and document addition.\")\n",
    "for k, coef in enumerate(topic_coefs):\n",
    "    print(f\"Topic {k}: {coef:.4f}\")\n",
    "\n",
    "\n",
    "# Get topic distributions for validation set to evaluate model performance\n",
    "# For sLDA, we need to calculate predictions and compare with actual values\n",
    "print(\"\\nCalculating model predictions...\")\n",
    "\n",
    "# Get regression coefficients from sLDA model\n",
    "raw_coef_output_pred = slda_model.get_regression_coef() # Renamed to avoid conflict if any subtle scope issue\n",
    "\n",
    "# Correctly extract 1D list of coefficients for prediction\n",
    "if isinstance(raw_coef_output_pred, (list, np.ndarray)) and \\\n",
    "   len(raw_coef_output_pred) == 1 and \\\n",
    "   isinstance(raw_coef_output_pred[0], (list, np.ndarray)) and \\\n",
    "   all(isinstance(c, (int, float, np.number)) for c in raw_coef_output_pred[0]):\n",
    "    # Handles [[c0, c1, ..., cN-1]]\n",
    "    coefficients_1d_pred = [float(c) for c in raw_coef_output_pred[0]]\n",
    "elif isinstance(raw_coef_output_pred, (list, np.ndarray)) and \\\n",
    "     all(isinstance(c, (int, float, np.number)) for c in raw_coef_output_pred):\n",
    "    # Handles [c0, c1, ..., cN-1] (already 1D)\n",
    "    coefficients_1d_pred = [float(c) for c in raw_coef_output_pred]\n",
    "else:\n",
    "    print(f\"Warning: Unexpected format for regression coefficients during prediction. Got: {raw_coef_output_pred}\")\n",
    "    coefficients_1d_pred = [] # Fallback\n",
    "\n",
    "# Define intercept and topic_coefs for prediction (mirroring the logic above)\n",
    "if len(coefficients_1d_pred) == num_topics:\n",
    "    intercept_pred = 0.0\n",
    "    topic_coefs_pred = coefficients_1d_pred\n",
    "elif len(coefficients_1d_pred) == num_topics + 1:\n",
    "    intercept_pred = coefficients_1d_pred[0]\n",
    "    topic_coefs_pred = coefficients_1d_pred[1:]\n",
    "else: # Fallback, should match the logic for non-pred variables\n",
    "    intercept_pred = 0.0\n",
    "    topic_coefs_pred = coefficients_1d_pred if coefficients_1d_pred else []\n",
    "\n",
    "# Ensure intercept and topic_coefs used in this prediction block are the _pred versions\n",
    "intercept = intercept_pred\n",
    "topic_coefs = topic_coefs_pred\n",
    "\n",
    "predicted_values = []\n",
    "actual_values = []\n",
    "\n",
    "for i, doc in enumerate(slda_model.docs):\n",
    "    topic_dist = doc.get_topic_dist()\n",
    "    # Ensure topic_dist is 1D and matches topic_coefs\n",
    "    pred = intercept + sum(coef * prob for coef, prob in zip(topic_coefs, topic_dist))\n",
    "    predicted_values.append(float(pred))\n",
    "    actual = float(ipi_values_norm[i])  # Already normalized above\n",
    "    actual_values.append(actual)\n",
    "\n",
    "# Convert to 1D numpy arrays for sklearn metrics\n",
    "#import numpy as np\n",
    "predicted_values = np.array(predicted_values).flatten()\n",
    "actual_values = np.array(actual_values).flatten()\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(actual_values, predicted_values)\n",
    "r2 = r2_score(actual_values, predicted_values)\n",
    "\n",
    "print(f\"Model evaluation: RÂ² = {r2:.4f}, MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After tokenization\n",
    "num_docs_with_tokens = sum(1 for tokens in docs_tokens if tokens)\n",
    "print(f\"Documents with tokens: {num_docs_with_tokens} / {len(docs_tokens)}\")\n",
    "print(\"Sample tokenized docs:\", docs_tokens[:5])\n",
    "\n",
    "# After adding documents\n",
    "print(f\"Number of documents in sLDA model: {len(slda_model.docs)}\")\n",
    "assert len(slda_model.docs) > 0, \"No documents were added to the sLDA model!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831166ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the topics and regression coefficients\n",
    "# Extract the top words for each topic\n",
    "topic_terms = {}\n",
    "for topic_idx in range(num_topics):\n",
    "    # Get top 20 words for this topic\n",
    "    top_terms = slda_model.get_topic_words(topic_idx, top_n=20)\n",
    "    topic_weight = topic_coefs[topic_idx]  # Get regression coefficient\n",
    "    topic_terms[topic_idx] = (top_terms, topic_weight)\n",
    "\n",
    "# Print out the topics and their regression coefficients\n",
    "print(\"\\nTopic words and regression coefficients (topic influence on IPI):\")\n",
    "for topic_idx, (terms, weight) in topic_terms.items():\n",
    "    print(f\"\\nTopic {topic_idx}:\")\n",
    "    print(f\"Regression coefficient: {weight:.4f}\")\n",
    "    print(f\"Top terms: {', '.join([term for term, _ in terms[:10]])}\")\n",
    "    \n",
    "# Print topic weights for predicting IPI\n",
    "print(\"\\nRegression coefficients (topic influence on IPI):\")\n",
    "for k, coef in enumerate(topic_coefs):\n",
    "    print(f\"Topic {k}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bb1da",
   "metadata": {},
   "source": [
    "### Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top topics and their relationship to the IPI values\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    # Get topic words and their weights\n",
    "    if idx < num_topics:  # Make sure we don't exceed the number of topics\n",
    "        # Get the top 50 words for this topic\n",
    "        top_words = slda_model.get_topic_words(idx, top_n=50)\n",
    "        \n",
    "        # Create dictionary for the wordcloud\n",
    "        topic_dict = {word: weight for word, weight in top_words}\n",
    "        \n",
    "        # Create wordcloud\n",
    "        wc = WordCloud(width=500, height=300, background_color='white', max_words=50)\n",
    "        wc.generate_from_frequencies(topic_dict)\n",
    "        ax.imshow(wc, interpolation='bilinear')\n",
    "        \n",
    "        # Add title with regression coefficient to show topic-IPI relationship\n",
    "        coef = topic_coefs[idx]\n",
    "        coef_sign = \"+\" if coef > 0 else \"\"\n",
    "        ax.set_title(f'Topic {idx + 1} (IPI Effect: {coef_sign}{coef:.3f})', fontsize=14)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')  # Turn off extra subplots if any\n",
    "\n",
    "plt.suptitle('Supervised LDA Topics â€“ Word Clouds with IPI Effect', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"../models/topic_model\", exist_ok=True)\n",
    "\n",
    "# Save the sLDA model using tomotopy's native save method\n",
    "slda_model.save(\"../models/topic_model/slda_model\")\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec527a",
   "metadata": {},
   "source": [
    "## Topic Extraction & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract document-topic distributions for each article\n",
    "doc_topic_dists = []\n",
    "for doc in slda_model.docs:\n",
    "    doc_topic_dists.append(doc.get_topic_dist())\n",
    "\n",
    "# Create topic column names\n",
    "topic_cols = [f'topic_{i}' for i in range(num_topics)]\n",
    "\n",
    "# Create a DataFrame with topic distributions\n",
    "df_topics = pd.DataFrame(doc_topic_dists, columns=topic_cols)\n",
    "\n",
    "# Combine with metadata\n",
    "df_meta = df_merged[['date', 'publication', 'month']].reset_index(drop=True).copy()  # Using existing columns\n",
    "# Only keep rows for which we have topic distributions (some might have been filtered out)\n",
    "df_meta = df_meta.iloc[:len(doc_topic_dists)].copy()\n",
    "\n",
    "df_combined = pd.concat([df_meta, df_topics], axis=1)\n",
    "\n",
    "# Aggregate topic shares by month and publisher\n",
    "df_monthly_pub = df_combined.groupby(['month', 'publication'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# Save to CSV with tomotopy_sLDA in the filename\n",
    "df_monthly_pub.to_csv('../data/processed/monthly_topic_shares_by_publisher_sLDA.csv', index=False)\n",
    "\n",
    "print(\"âœ… Saved: 'monthly_topic_shares_by_publisher_sLDA.csv'\")\n",
    "\n",
    "# Display the head of the saved CSV file\n",
    "df_check_csv = pd.read_csv('../data/processed/monthly_topic_shares_by_publisher_sLDA.csv')\n",
    "print(\"Head of 'monthly_topic_shares_by_publisher_sLDA.csv':\")\n",
    "df_check_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d739732",
   "metadata": {},
   "source": [
    "### Compare topic trends with IPI values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the relationship between topic prevalence and IPI values\n",
    "\n",
    "# Get average IPI by month\n",
    "monthly_ipi = df_ipi.groupby('month')['ipi_value'].mean().reset_index()\n",
    "\n",
    "# Get average topic distributions by month (across all publishers)\n",
    "monthly_topics = df_monthly_pub.groupby('month')[topic_cols].mean().reset_index()\n",
    "\n",
    "# Merge the data\n",
    "df_trends = pd.merge(monthly_topics, monthly_ipi, on='month', how='inner')\n",
    "\n",
    "# Plot the trends of the top 3 most influential topics (based on regression coefficients)\n",
    "topic_importance = [(i, abs(c)) for i, c in enumerate(topic_coefs)]\n",
    "top_topics = sorted(topic_importance, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot IPI values\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "months = df_trends['month'].astype(str).tolist()\n",
    "ax1.plot(months, df_trends['ipi_value'], 'k-', label='IPI Value')\n",
    "\n",
    "# Plot topic trends\n",
    "for topic_idx, importance in top_topics:\n",
    "    topic_col = f'topic_{topic_idx}'\n",
    "    coef = topic_coefs[topic_idx]\n",
    "    coef_sign = \"+\" if coef > 0 else \"\"\n",
    "    ax2.plot(months, df_trends[topic_col], '--', \n",
    "            label=f'Topic {topic_idx} (IPI Effect: {coef_sign}{coef:.3f})')\n",
    "\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('IPI Value')\n",
    "ax2.set_ylabel('Topic Prevalence')\n",
    "ax1.set_xticklabels(months, rotation=90)\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('IPI Values and Top Topic Trends Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f0cb1",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "The output CSV file (`monthly_topic_shares_by_publisher_sLDA.csv`) contains the monthly topic distributions for each publisher, which can be used for further analysis or visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
